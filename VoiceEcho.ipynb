{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d467d0a4-5803-4a37-b55c-9803af55e885",
   "metadata": {},
   "outputs": [],
   "source": [
    "import utils\n",
    "\n",
    "audioFileOutput = \"audio_ouput.mp3\"\n",
    "text = \"Hello World! Hear me speak!\"\n",
    "\n",
    "audioFileInput = \"input4.wav\" \n",
    "\n",
    "utils.setupOpenAIAPI()\n",
    "utils.textToAudioFile(text, audioFileOutput) # Convert example text to audio file as test\n",
    "\n",
    "# transcription = utils.audioToText(audioFileInput)\n",
    "# print(transcription)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "639dc704-15e9-413a-b83b-eda6e51323eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import utils  # Make sure you have a utils.py file for your OpenAI API interactions\n",
    "import sounddevice as sd\n",
    "import scipy.io.wavfile as wav\n",
    "import numpy as np\n",
    "import webrtcvad\n",
    "from pydub import AudioSegment\n",
    "from pydub.playback import play\n",
    "import collections\n",
    "import time\n",
    "\n",
    "# Initialize OpenAI API \n",
    "# (replace with your actual API key setup from utils.py)\n",
    "utils.setupOpenAIAPI() \n",
    "\n",
    "# Define audio parameters\n",
    "fs = 16000 \n",
    "frame_duration_ms = 30 \n",
    "\n",
    "def record_audio_vad(filename):\n",
    "    vad = webrtcvad.Vad(1)\n",
    "    num_padding_frames = 10 \n",
    "    ring_buffer = collections.deque(maxlen=num_padding_frames)\n",
    "    triggered = False\n",
    "    voiced_frames = []\n",
    "    print(\"Recording... Speak now.\")\n",
    "\n",
    "    silence_threshold_seconds = 1.5 \n",
    "    last_voice_timestamp = time.time()\n",
    "\n",
    "    with sd.InputStream(samplerate=fs, channels=1, dtype='int16') as stream:\n",
    "        while True:\n",
    "            audio_chunk, overflowed = stream.read(int(fs * frame_duration_ms / 1000))\n",
    "            is_speech = vad.is_speech(audio_chunk.tobytes(), fs)\n",
    "\n",
    "            if is_speech:\n",
    "                last_voice_timestamp = time.time()\n",
    "\n",
    "            if not triggered:\n",
    "                ring_buffer.append((audio_chunk, is_speech))\n",
    "                num_voiced = len([f for f, speech in ring_buffer if speech])\n",
    "                if num_voiced > 0.9 * ring_buffer.maxlen:\n",
    "                    triggered = True\n",
    "                    print(\"Recording started.\")\n",
    "                    voiced_frames.extend([f for f, s in ring_buffer])\n",
    "                    ring_buffer.clear()\n",
    "            else:\n",
    "                voiced_frames.append(audio_chunk)\n",
    "                ring_buffer.append((audio_chunk, is_speech))\n",
    "                num_unvoiced = len([f for f, speech in ring_buffer if not speech])\n",
    "                \n",
    "                if num_unvoiced > 0.9 * (ring_buffer.maxlen) and (time.time() - last_voice_timestamp > silence_threshold_seconds):\n",
    "                    print(\"Recording stopped.\")\n",
    "                    break\n",
    "\n",
    "    audio_data = np.concatenate(voiced_frames, axis=0)\n",
    "    wav.write(filename, fs, audio_data)\n",
    "    print(\"Recording complete.\")\n",
    "\n",
    "def transcribe_audio(filename):\n",
    "    # Replace with your actual transcription logic from utils.py\n",
    "    return utils.audioToText(filename) \n",
    "\n",
    "def synthesize_speech(text, output_filename):\n",
    "    # Replace with your actual text-to-speech logic from utils.py\n",
    "    utils.textToAudioFile(text, output_filename) \n",
    "\n",
    "def play_audio(filename):\n",
    "    audio = AudioSegment.from_mp3(filename)\n",
    "    play(audio)\n",
    "\n",
    "# --- Main Script ---\n",
    "input_audio_file = \"input4.wav\"\n",
    "output_audio_file = \"output4.mp3\"\n",
    "\n",
    "record_audio_vad(input_audio_file)\n",
    "transcription = transcribe_audio(input_audio_file)\n",
    "print(\"Transcription:\", transcription)\n",
    "\n",
    "synthesize_speech(transcription, output_audio_file)\n",
    "play_audio(output_audio_file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06e12332-e5cf-4f4c-933e-b98bd5f2da65",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
